{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"haiku.rag","text":"<p><code>haiku.rag</code> is a Retrieval-Augmented Generation (RAG) library built to work on SQLite alone without the need for external vector databases. It uses sqlite-vec for storing the embeddings and performs semantic (vector) search as well as full-text search combined through Reciprocal Rank Fusion. Both open-source (Ollama) as well as commercial (OpenAI, VoyageAI) embedding providers are supported.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Local SQLite: No need to run additional servers</li> <li>Support for various embedding providers: Ollama, VoyageAI, OpenAI or add your own</li> <li>Hybrid Search: Vector search using <code>sqlite-vec</code> combined with full-text search <code>FTS5</code>, using Reciprocal Rank Fusion</li> <li>Question Answering: Built-in QA agents using Ollama, OpenAI, or Anthropic.</li> <li>File monitoring: Automatically index files when run as a server</li> <li>Extended file format support: Parse 40+ file formats including PDF, DOCX, HTML, Markdown, audio and more. Or add a URL!</li> <li>MCP server: Exposes functionality as MCP tools</li> <li>CLI commands: Access all functionality from your terminal</li> <li>Python client: Call <code>haiku.rag</code> from your own python applications</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install haiku.rag: <pre><code>uv pip install haiku.rag\n</code></pre></p> <p>Use from Python: <pre><code>from haiku.rag.client import HaikuRAG\n\nasync with HaikuRAG(\"database.db\") as client:\n    # Add a document\n    doc = await client.create_document(\"Your content here\")\n\n    # Search documents\n    results = await client.search(\"query\")\n\n    # Ask questions\n    answer = await client.ask(\"Who is the author of haiku.rag?\")\n</code></pre></p> <p>Or use the CLI: <pre><code>haiku-rag add \"Your document content\"\nhaiku-rag search \"query\"\nhaiku-rag ask \"Who is the author of haiku.rag?\"\n</code></pre></p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation - Install haiku.rag with different providers</li> <li>Configuration - Environment variables and settings</li> <li>CLI - Command line interface usage</li> <li>Question Answering - QA agents and natural language queries</li> <li>Server - File monitoring and server mode</li> <li>MCP - Model Context Protocol integration</li> <li>Python - Python API reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We use the repliqa dataset for the evaluation of <code>haiku.rag</code>.</p> <p>You can perform your own evaluations using as example the script found at <code>tests/generate_benchmark_db.py</code>.</p>"},{"location":"benchmarks/#recall","title":"Recall","text":"<p>In order to calculate recall, we load the <code>News Stories</code> from <code>repliqa_3</code> which is 1035 documents and index them in a sqlite db. Subsequently, we run a search over the <code>question</code> field for each row of the dataset and check whether we match the document that answers the question.</p> <p>The recall obtained is ~0.73 for matching in the top result, raising to ~0.75 for the top 3 results.</p> Model Document in top 1 Document in top 3 Ollama / <code>mxbai-embed-large</code> 0.77 0.89 Ollama / <code>nomic-embed-text</code> 0.74 0.88 OpenAI / <code>text-embeddings-3-small</code> 0.75 0.88"},{"location":"benchmarks/#questionanswer-evaluation","title":"Question/Answer evaluation","text":"<p>Again using the same dataset, we use a QA agent to answer the question. In addition we use an LLM judge (using the Ollama <code>qwen3</code>) to evaluate whether the answer is correct or not. The obtained accuracy is as follows:</p> Embedding Model QA Model Accuracy Ollama / <code>mxbai-embed-large</code> Ollama / <code>qwen3</code> 0.64 Ollama / <code>mxbai-embed-large</code> Anthropic / <code>Claude Sonnet 3.7</code> 0.79"},{"location":"cli/","title":"Command Line Interface","text":"<p>The <code>haiku-rag</code> CLI provides complete document management functionality.</p>"},{"location":"cli/#document-management","title":"Document Management","text":""},{"location":"cli/#list-documents","title":"List Documents","text":"<pre><code>haiku-rag list\n</code></pre>"},{"location":"cli/#add-documents","title":"Add Documents","text":"<p>From text: <pre><code>haiku-rag add \"Your document content here\"\n</code></pre></p> <p>From file or URL: <pre><code>haiku-rag add-src /path/to/document.pdf\nhaiku-rag add-src https://example.com/article.html\n</code></pre></p>"},{"location":"cli/#get-document","title":"Get Document","text":"<pre><code>haiku-rag get 1\n</code></pre>"},{"location":"cli/#delete-document","title":"Delete Document","text":"<pre><code>haiku-rag delete 1\n</code></pre>"},{"location":"cli/#rebuild-database","title":"Rebuild Database","text":"<p>Rebuild the database by deleting all chunks &amp; embeddings and re-indexing all documents:</p> <pre><code>haiku-rag rebuild\n</code></pre> <p>Use this when you want to change things like the embedding model or chunk size for example.</p>"},{"location":"cli/#search","title":"Search","text":"<p>Basic search: <pre><code>haiku-rag search \"machine learning\"\n</code></pre></p> <p>With options: <pre><code>haiku-rag search \"python programming\" --limit 10 --k 100\n</code></pre></p>"},{"location":"cli/#question-answering","title":"Question Answering","text":"<p>Ask questions about your documents: <pre><code>haiku-rag ask \"Who is the author of haiku.rag?\"\n</code></pre></p> <p>The QA agent will search your documents for relevant information and provide a comprehensive answer.</p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>View current configuration settings: <pre><code>haiku-rag settings\n</code></pre></p>"},{"location":"cli/#server","title":"Server","text":"<p>Start the MCP server: <pre><code># HTTP transport (default)\nhaiku-rag serve\n\n# stdio transport\nhaiku-rag serve --stdio\n\n# SSE transport\nhaiku-rag serve --sse\n</code></pre></p>"},{"location":"cli/#options","title":"Options","text":"<p>All commands support: - <code>--db</code> - Specify custom database path - <code>-h</code> - Show help for specific command</p> <p>Example: <pre><code>haiku-rag list --db /path/to/custom.db\nhaiku-rag add -h\n</code></pre></p>"},{"location":"configuration/","title":"Configuration","text":"<p>Configuration is done through the use of environment variables.</p> <p>Note</p> <p>If you create a db with certain settings and later change them, <code>haiku.rag</code> will detect incompatibilities (for example, if you change embedding provider) and will exit. You can rebuild the database to apply the new settings, see Rebuild Database.</p>"},{"location":"configuration/#file-monitoring","title":"File Monitoring","text":"<p>Set directories to monitor for automatic indexing:</p> <pre><code># Monitor single directory\nMONITOR_DIRECTORIES=\"/path/to/documents\"\n\n# Monitor multiple directories\nMONITOR_DIRECTORIES=\"/path/to/documents,/another_path/to/documents\"\n</code></pre>"},{"location":"configuration/#embedding-providers","title":"Embedding Providers","text":"<p>If you use Ollama, you can use any pulled model that supports embeddings.</p>"},{"location":"configuration/#ollama-default","title":"Ollama (Default)","text":"<pre><code>EMBEDDINGS_PROVIDER=\"ollama\"\nEMBEDDINGS_MODEL=\"mxbai-embed-large\"\nEMBEDDINGS_VECTOR_DIM=1024\n</code></pre>"},{"location":"configuration/#voyageai","title":"VoyageAI","text":"<p>If you want to use VoyageAI embeddings you will need to install <code>haiku.rag</code> with the VoyageAI extras,</p> <pre><code>uv pip install haiku.rag --extra voyageai\n</code></pre> <pre><code>EMBEDDINGS_PROVIDER=\"voyageai\"\nEMBEDDINGS_MODEL=\"voyage-3.5\"\nEMBEDDINGS_VECTOR_DIM=1024\nVOYAGE_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"configuration/#openai","title":"OpenAI","text":"<p>If you want to use OpenAI embeddings you will need to install <code>haiku.rag</code> with the VoyageAI extras,</p> <pre><code>uv pip install haiku.rag --extra openai\n</code></pre> <p>and set environment variables.</p> <pre><code>EMBEDDINGS_PROVIDER=\"openai\"\nEMBEDDINGS_MODEL=\"text-embedding-3-small\"  # or text-embedding-3-large\nEMBEDDINGS_VECTOR_DIM=1536\nOPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"configuration/#question-answering-providers","title":"Question Answering Providers","text":"<p>Configure which LLM provider to use for question answering.</p>"},{"location":"configuration/#ollama-default_1","title":"Ollama (Default)","text":"<pre><code>QA_PROVIDER=\"ollama\"\nQA_MODEL=\"qwen3\"\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n</code></pre>"},{"location":"configuration/#openai_1","title":"OpenAI","text":"<p>For OpenAI QA, you need to install haiku.rag with OpenAI extras:</p> <pre><code>uv pip install haiku.rag --extra openai\n</code></pre> <p>Then configure:</p> <pre><code>QA_PROVIDER=\"openai\"\nQA_MODEL=\"gpt-4o-mini\"  # or gpt-4, gpt-3.5-turbo, etc.\nOPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"configuration/#anthropic","title":"Anthropic","text":"<p>For Anthropic QA, you need to install haiku.rag with Anthropic extras:</p> <pre><code>uv pip install haiku.rag --extra anthropic\n</code></pre> <p>Then configure:</p> <pre><code>QA_PROVIDER=\"anthropic\"\nQA_MODEL=\"claude-3-5-haiku-20241022\"  # or claude-3-5-sonnet-20241022, etc.\nANTHROPIC_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"configuration/#other-settings","title":"Other Settings","text":""},{"location":"configuration/#database-and-storage","title":"Database and Storage","text":"<pre><code># Default data directory (where SQLite database is stored)\nDEFAULT_DATA_DIR=\"/path/to/data\"\n</code></pre>"},{"location":"configuration/#document-processing","title":"Document Processing","text":"<pre><code># Chunk size for document processing\nCHUNK_SIZE=256\n\n# Chunk overlap for better context\nCHUNK_OVERLAP=32\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<pre><code>uv pip install haiku.rag\n</code></pre> <p>By default, Ollama (with the <code>mxbai-embed-large</code> model) is used for embeddings.</p>"},{"location":"installation/#provider-specific-installation","title":"Provider-Specific Installation","text":"<p>For other embedding providers, install with extras:</p>"},{"location":"installation/#voyageai","title":"VoyageAI","text":"<pre><code>uv pip install haiku.rag --extra voyageai\n</code></pre>"},{"location":"installation/#openai","title":"OpenAI","text":"<pre><code>uv pip install haiku.rag --extra openai\n</code></pre>"},{"location":"installation/#anthropic","title":"Anthropic","text":"<pre><code>uv pip install haiku.rag --extra anthropic\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>SQLite 3.38+</li> <li>Ollama (for default embeddings)</li> </ul>"},{"location":"mcp/","title":"Model Context Protocol (MCP)","text":"<p>The MCP server exposes <code>haiku.rag</code> as MCP tools for compatible MCP clients.</p>"},{"location":"mcp/#available-tools","title":"Available Tools","text":""},{"location":"mcp/#document-management","title":"Document Management","text":"<ul> <li><code>add_document_from_file</code> - Add documents from local file paths</li> <li><code>add_document_from_url</code> - Add documents from URLs</li> <li><code>add_document_from_text</code> - Add documents from raw text content</li> <li><code>get_document</code> - Retrieve specific documents by ID</li> <li><code>list_documents</code> - List all documents with pagination</li> <li><code>delete_document</code> - Delete documents by ID</li> </ul>"},{"location":"mcp/#search","title":"Search","text":"<ul> <li><code>search_documents</code> - Search documents using hybrid search (vector + full-text)</li> </ul>"},{"location":"mcp/#starting-mcp-server","title":"Starting MCP Server","text":"<p>The MCP server starts automatically with the serve command and supports <code>Streamable HTTP</code>, <code>stdio</code> and <code>SSE</code> transports:</p> <pre><code># Default HTTP transport\nhaiku-rag serve\n\n# stdio transport (for Claude Desktop)\nhaiku-rag serve --stdio\n\n# SSE transport\nhaiku-rag serve --sse\n</code></pre>"},{"location":"python/","title":"Python API","text":"<p>Use <code>haiku.rag</code> directly in your Python applications.</p>"},{"location":"python/#basic-usage","title":"Basic Usage","text":"<pre><code>from pathlib import Path\nfrom haiku.rag.client import HaikuRAG\n\n# Use as async context manager (recommended)\nasync with HaikuRAG(\"path/to/database.db\") as client:\n    # Your code here\n    pass\n</code></pre>"},{"location":"python/#document-management","title":"Document Management","text":""},{"location":"python/#creating-documents","title":"Creating Documents","text":"<p>From text: <pre><code>doc = await client.create_document(\n    content=\"Your document content here\",\n    uri=\"doc://example\",\n    metadata={\"source\": \"manual\", \"topic\": \"example\"}\n)\n</code></pre></p> <p>From file: <pre><code>doc = await client.create_document_from_source(\"path/to/document.pdf\")\n</code></pre></p> <p>From URL: <pre><code>doc = await client.create_document_from_source(\"https://example.com/article.html\")\n</code></pre></p>"},{"location":"python/#retrieving-documents","title":"Retrieving Documents","text":"<p>By ID: <pre><code>doc = await client.get_document_by_id(1)\n</code></pre></p> <p>By URI: <pre><code>doc = await client.get_document_by_uri(\"file:///path/to/document.pdf\")\n</code></pre></p> <p>List all documents: <pre><code>docs = await client.list_documents(limit=10, offset=0)\n</code></pre></p>"},{"location":"python/#updating-documents","title":"Updating Documents","text":"<pre><code>doc.content = \"Updated content\"\nawait client.update_document(doc)\n</code></pre>"},{"location":"python/#deleting-documents","title":"Deleting Documents","text":"<pre><code>await client.delete_document(doc.id)\n</code></pre>"},{"location":"python/#rebuilding-the-database","title":"Rebuilding the Database","text":"<pre><code>async for doc_id in client.rebuild_database():\n    print(f\"Processed document {doc_id}\")\n</code></pre>"},{"location":"python/#searching-documents","title":"Searching Documents","text":"<p>Basic search: <pre><code>results = await client.search(\"machine learning algorithms\", limit=5)\nfor chunk, score in results:\n    print(f\"Score: {score:.3f}\")\n    print(f\"Content: {chunk.content}\")\n    print(f\"Document ID: {chunk.document_id}\")\n</code></pre></p> <p>With options: <pre><code>results = await client.search(\n    query=\"machine learning\",\n    limit=5,  # Maximum results to return\n    k=60      # RRF parameter for reciprocal rank fusion\n)\n\n# Process results\nfor chunk, relevance_score in results:\n    print(f\"Relevance: {relevance_score:.3f}\")\n    print(f\"Content: {chunk.content}\")\n    print(f\"From document: {chunk.document_id}\")\n    print(f\"Document URI: {chunk.document_uri}\")\n    print(f\"Document metadata: {chunk.document_meta}\")\n</code></pre></p>"},{"location":"python/#question-answering","title":"Question Answering","text":"<p>Ask questions about your documents:</p> <pre><code>answer = await client.ask(\"Who is the author of haiku.rag?\")\nprint(answer)\n</code></pre> <p>The QA agent will search your documents for relevant information and use the configured LLM to generate a comprehensive answer.</p> <p>The QA provider and model can be configured via environment variables (see Configuration).</p>"},{"location":"server/","title":"Server Mode","text":"<p>The server provides automatic file monitoring and MCP functionality.</p>"},{"location":"server/#starting-the-server","title":"Starting the Server","text":"<pre><code>haiku-rag serve\n</code></pre> <p>Transport options: - <code>--http</code> (default) - Streamable HTTP transport - <code>--stdio</code> - Standard input/output transport - <code>--sse</code> - Server-sent events transport</p>"},{"location":"server/#file-monitoring","title":"File Monitoring","text":"<p>Set <code>MONITOR_DIRECTORIES</code> environment variable to enable automatic file monitoring:</p> <pre><code>export MONITOR_DIRECTORIES=\"/path/to/documents\"\nhaiku-rag serve\n</code></pre>"},{"location":"server/#monitoring-features","title":"Monitoring Features","text":"<ul> <li>Startup: Scans all monitored directories and adds new files</li> <li>File Added/Modified: Automatically parses and updates documents</li> <li>File Deleted: Removes corresponding documents from database</li> </ul>"},{"location":"server/#supported-formats","title":"Supported Formats","text":"<p>The server can parse 40+ file formats including: - PDF documents - Microsoft Office (DOCX, XLSX, PPTX) - HTML and Markdown - Plain text files - Audio files - And more...</p> <p>URLs are also supported for web content.</p>"}]}